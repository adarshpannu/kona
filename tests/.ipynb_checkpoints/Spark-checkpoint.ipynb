{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- emp_dept_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EMPFILE = \"/Users/adarshrp/Projects/yard/data/emp.csv\"\n",
    "emp = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(EMPFILE)\n",
    "emp.printSchema()\n",
    "emp.createOrReplaceTempView(\"emp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+\n",
      "|(CAST(sum(CAST((age + 10) AS BIGINT)) AS DOUBLE) / CAST(count(1) AS DOUBLE))|\n",
      "+----------------------------------------------------------------------------+\n",
      "|                                                                        47.0|\n",
      "|                                                          42.666666666666664|\n",
      "|                                                                       41.75|\n",
      "|                                                                        47.4|\n",
      "|                                                          54.333333333333336|\n",
      "+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select sum(age + 10) / count(*) from EMP\n",
    "group by emp_dept_id\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "WITH CTE1 AS (select '--cte1', age, '--cte1' from emp),\n",
    " CTE2 AS (select '--cte2', age, '--cte2' from emp)\n",
    "SELECT * FROM CTE1, (select '2nd', dept_id, name from EMP) CTE1\n",
    "WHERE name in (select name from emp CTE1 where CTE1.age >= 0) \n",
    "and age in (select CTE2.age from EMP CTE2 where CTE1.age = CTE2.age)\n",
    "\"\"\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(\"/tmp/file.parquet\")\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[column_1: bigint, column_2: string, column_3: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquetFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['column_1', 'column_2', 'column_3']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquetFile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+\n",
      "|column_1|   column_2|            column_3|\n",
      "+--------+-----------+--------------------+\n",
      "|       1|    AMERICA|hs use ironic, ev...|\n",
      "|       2|       ASIA|ges. thinly even ...|\n",
      "|       3|     EUROPE|ly final courts c...|\n",
      "|       4|MIDDLE EAST|uickly special ac...|\n",
      "+--------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM parquetFile\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquetFile.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile.write.format(\"parquet\").partitionBy(\"column_2\").mode(\"overwrite\").save(\"/tmp/pqptns.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TPCH Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mConvert.ipynb\u001b[m\u001b[m     datatypes.csv     emp.parquet       userdata1.parquet\r\n",
      "R.csv             dept.csv          \u001b[34mempdir\u001b[m\u001b[m\r\n",
      "S.csv             dept_details.csv  \u001b[34mnorthwind\u001b[m\u001b[m\r\n",
      "T.csv             emp.csv           \u001b[34mtpch0.01\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(\"/Users/adarshrp/Downloads/part-0.parquet\")\n",
    "parquetFile.createOrReplaceTempView(\"LINEITEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[l_orderkey: bigint, l_partkey: bigint, l_suppkey: bigint, l_linenumber: int, l_quantity: double, l_extendedprice: double, l_discount: double, l_tax: double, l_returnflag: string, l_linestatus: string, l_shipdate: date, l_commitdate: date, l_receiptdate: date, l_shipinstruct: string, l_shipmode: string, l_comment: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquetFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1110933221.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/m9/t4qs9byx51x4gl0zwfp6v60w0000gn/T/ipykernel_52764/1110933221.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    SELECT l_returnflag, L_LINESTATUS, SUM(L_QUANTITY) AS SUM_QTY, MAX(L_LINESTATUS), MIN(L_LINESTATUS)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "SELECT L_RETURNFLAG, L_LINESTATUS, SUM(L_QUANTITY) AS SUM_QTY, MAX(L_LINESTATUS), MIN(L_LINESTATUS)\n",
    "FROM LINEITEM\n",
    "--WHERE L_SHIPDATE <= CAST('1992-01-03' AS DATE32)\n",
    "GROUP BY L_RETURNFLAG, L_LINESTATUS;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(sql).show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
